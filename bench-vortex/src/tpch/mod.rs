use std::fs;
use std::path::Path;
use std::sync::Arc;

use anyhow::bail;
use arrow_array::StructArray as ArrowStructArray;
use arrow_schema::Schema;
use datafusion::datasource::MemTable;
use datafusion::prelude::{CsvReadOptions, SessionContext};
use itertools::Itertools;
use object_store::ObjectStore;
use url::Url;
use vortex::arrays::ChunkedArray;
use vortex::arrow::FromArrowArray;
use vortex::{Array, ArrayRef, IntoArray, TryIntoArray};
use vortex_datafusion::SessionContextExt;

use crate::engines::df::{get_session_context, make_object_store};
use crate::{BenchmarkDataset, Format, datasets};

pub mod dbgen;
pub mod duckdb;
mod execute;
pub mod schema;

pub use execute::*;

pub const TPC_H_ROW_COUNT_ARRAY_LENGTH: usize = 23;
pub const EXPECTED_ROW_COUNTS_SF1: [usize; TPC_H_ROW_COUNT_ARRAY_LENGTH] = [
    // The 0th entry is a dummy so that Query 1's row count is at index 1.
    0, 4, 460, 11620, 5, 5, 1, 4, 2, 175, 37967, 1048, 2, 42, 1, 1, 18314, 1, 57, 1, 186, 411, 7,
];
pub const EXPECTED_ROW_COUNTS_SF10: [usize; TPC_H_ROW_COUNT_ARRAY_LENGTH] = [
    // The 0th entry is a dummy so that Query 1's row count is at index 1.
    //
    // Generated by executing the SQL in each query file using duckdb with the table names replaced
    // by "$NAME.parquet".
    0, 4, 4667, 114003, 5, 5, 1, 4, 2, 175, 381105, 0, 2, 46, 1, 1, 27840, 1, 624, 1, 1804, 4009, 7,
];

// Generate table dataset.
pub async fn load_datasets(
    base_dir: &Url,
    format: Format,
    dataset: BenchmarkDataset,
    disable_datafusion_cache: bool,
) -> anyhow::Result<SessionContext> {
    let context = get_session_context(disable_datafusion_cache);

    let object_store = make_object_store(&context, base_dir)?;

    let files = match dataset {
        BenchmarkDataset::TpcH => vec![
            ("customer", Some(schema::CUSTOMER.clone())),
            ("lineitem", Some(schema::LINEITEM.clone())),
            ("nation", Some(schema::NATION.clone())),
            ("orders", Some(schema::ORDERS.clone())),
            ("part", Some(schema::PART.clone())),
            ("partsupp", Some(schema::PARTSUPP.clone())),
            ("region", Some(schema::REGION.clone())),
            ("supplier", Some(schema::SUPPLIER.clone())),
        ],

        BenchmarkDataset::TpcDS => BenchmarkDataset::TpcDS
            .tables()
            .iter()
            .map(|f| (*f, None))
            .collect_vec(),
        BenchmarkDataset::ClickBench { .. } => todo!(),
    };

    for (name, path, schema) in files.into_iter().map(|(name, schema)| {
        let format = if format == Format::Arrow {
            Format::Csv
        } else {
            format
        };
        (
            name,
            base_dir.join(&format!("{}/{name}.{}", format.name(), format.ext())),
            schema,
        )
    }) {
        let path = path?;
        match format {
            Format::Csv => register_csv(&context, name, &path, schema).await?,
            Format::Arrow => register_arrow(&context, name, &path, schema).await?,
            Format::Parquet => {
                register_parquet(&context, object_store.clone(), name, &path, schema).await?
            }
            Format::InMemoryVortex => register_vortex(&context, name, &path, schema).await?,
            Format::OnDiskVortex => {
                register_vortex_file(&context, object_store.clone(), name, &path, schema).await?
            }
            Format::OnDiskDuckDB => unreachable!("duckdb never supported with datafusion"),
        }
    }

    Ok(context)
}

pub mod named_locks {
    use std::future::Future;
    use std::sync::{Arc, LazyLock};

    use tokio::sync::Mutex;
    use vortex::aliases::hash_map::HashMap;

    type NamedLocksMap = LazyLock<Mutex<HashMap<String, Arc<Mutex<()>>>>>;
    static NAMED_LOCKS: NamedLocksMap = LazyLock::new(|| Mutex::new(HashMap::new()));

    pub async fn get(name: String) -> Arc<Mutex<()>> {
        let named_locks = &mut *NAMED_LOCKS.lock().await;
        let mutex: &Arc<Mutex<()>> = named_locks
            .entry(name)
            .or_insert_with(|| Arc::new(Mutex::new(())));
        mutex.clone()
    }

    pub async fn with_lock<F, T, E>(name: String, f: impl FnOnce() -> F) -> Result<T, E>
    where
        F: Future<Output = Result<T, E>>,
    {
        let lock = get(name).await;
        let _guard = lock.lock().await;
        f().await
    }
}

async fn register_csv(
    session: &SessionContext,
    name: &str,
    file: &Url,
    schema: Option<Schema>,
) -> anyhow::Result<()> {
    let Some(schema) = schema else {
        bail!("cannot run bench with format==csv")
    };
    session
        .register_csv(
            name,
            file.as_str(),
            CsvReadOptions::default()
                .delimiter(b'|')
                .has_header(false)
                .file_extension("csv")
                .schema(&schema),
        )
        .await?;

    Ok(())
}

async fn register_arrow(
    session: &SessionContext,
    name: &str,
    file: &Url,
    schema: Option<Schema>,
) -> anyhow::Result<()> {
    let Some(schema) = schema else {
        bail!("cannot have an arrow run without a preloaded schema")
    };

    // Read CSV file into a set of Arrow RecordBatch.
    let record_batches = session
        .read_csv(
            file.as_str(),
            CsvReadOptions::default()
                .delimiter(b'|')
                .has_header(false)
                .file_extension("csv")
                .schema(&schema),
        )
        .await?
        .collect()
        .await?;

    let mem_table = MemTable::try_new(Arc::new(schema.clone()), vec![record_batches])?;
    session.register_table(name, Arc::new(mem_table))?;

    Ok(())
}

async fn register_parquet(
    session: &SessionContext,
    object_store: Arc<dyn ObjectStore>,
    name: &str,
    file: &Url,
    schema: Option<Schema>,
) -> anyhow::Result<()> {
    datasets::file::register_parquet_files(
        session,
        object_store,
        name,
        file,
        schema,
        BenchmarkDataset::TpcH,
    )
    .await
}

async fn register_vortex_file(
    session: &SessionContext,
    object_store: Arc<dyn ObjectStore>,
    table_name: &str,
    file: &Url,
    schema: Option<Schema>,
) -> anyhow::Result<()> {
    datasets::file::register_vortex_files(
        session,
        object_store,
        table_name,
        file,
        schema,
        BenchmarkDataset::TpcH,
    )
    .await
}

async fn register_vortex(
    session: &SessionContext,
    name: &str,
    file: &Url,
    schema: Option<Schema>,
) -> anyhow::Result<()> {
    let Some(schema) = schema else {
        bail!("required schema for in mem vortex")
    };
    // TODO(joe): use parquet for speed?
    let record_batches = session
        .read_csv(
            file.as_str(),
            CsvReadOptions::default()
                .delimiter(b'|')
                .has_header(false)
                .file_extension("csv")
                .schema(&schema),
        )
        .await?
        .collect()
        .await?;

    // Create a ChunkedArray from the set of chunks.
    let chunks: Vec<ArrayRef> = record_batches
        .into_iter()
        .map(ArrowStructArray::from)
        .map(|struct_array| ArrayRef::from_arrow(&struct_array, false))
        .collect();

    let dtype = chunks[0].dtype().clone();
    let chunked_array = ChunkedArray::try_new(chunks, dtype)?.into_array();

    session.register_mem_vortex(name, chunked_array)?;

    Ok(())
}

/// Load a table as an uncompressed Vortex array.
pub async fn load_table(data_dir: impl AsRef<Path>, name: &str, schema: &Schema) -> ArrayRef {
    // Create a local session to load the CSV file from the path.
    let path = data_dir.as_ref().join(name).with_extension("csv");
    let record_batches = SessionContext::new()
        .read_csv(
            path.to_str().unwrap(),
            CsvReadOptions::default()
                .delimiter(b'|')
                .has_header(false)
                .file_extension("csv")
                .schema(schema),
        )
        .await
        .unwrap()
        .collect()
        .await
        .unwrap();

    let chunks = record_batches
        .into_iter()
        .map(|batch| batch.try_into_array().unwrap());

    ChunkedArray::from_iter(chunks).into_array()
}

pub fn tpch_queries() -> impl Iterator<Item = (usize, Vec<String>)> {
    (1..=22).map(|q| (q, tpch_query(q)))
}

// A few tpch queries have multiple statements, this handles that
fn tpch_query(query_idx: usize) -> Vec<String> {
    let manifest_dir = Path::new(env!("CARGO_MANIFEST_DIR"))
        .join("tpch")
        .join(format!("q{}", query_idx))
        .with_extension("sql");
    fs::read_to_string(manifest_dir)
        .unwrap()
        .split(';')
        .map(|s| s.trim())
        .filter(|s| !s.is_empty())
        .map(|s| s.to_string())
        .collect()
}
